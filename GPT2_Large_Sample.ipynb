{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dLKTm3x1kSv"
      },
      "source": [
        "# How to Write the First Page of Your Paper with Python\n",
        "An illustration using a siple GPT2 model. GPT3 has obviously reduced this process to n API call."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTyaL2Fe1q8h"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgJZRqjZ2FPj",
        "outputId": "766f6153-d263-42e7-c62b-5cb8733e2cf8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        }
      },
      "source": [
        "import re\n",
        "\n",
        "import requests\n",
        "import torch\n",
        "\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGnHu99R2Mpd"
      },
      "source": [
        "# the following may take a few minutes for the download!\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-large')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2-large').to('cuda')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's finetune the model on *Crime and Punishment*"
      ],
      "metadata": {
        "id": "oFrqss7SfWSf"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7U8Nh4Hzv0Na"
      },
      "source": [
        "crime_n_punish = requests.get('http://www.gutenberg.org/files/2554/2554-0.txt').text\n",
        "crime_n_punish = re.sub(r'[\\n\\r]+', ' ', crime_n_punish)\n",
        "sentences = [s for s in re.findall(r'[^.?!]+[.?!]', crime_n_punish) if len(s) >= 20]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vetkSYQuyjr"
      },
      "source": [
        "model.train()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = Adam(model.parameters(), lr=3e-5)\n",
        "\n",
        "optimizer.zero_grad()\n",
        "\n",
        "for epoch in range(4):\n",
        "\n",
        "    running_loss = 0\n",
        "\n",
        "    for i, sentence in enumerate(sentences[1000:1900]):\n",
        "\n",
        "        tokenized = tokenizer.encode(sentence)\n",
        "        index = torch.randint(2, len(tokenized) - 1, (1,)).item()\n",
        "        tokenized = tokenized[:index]\n",
        "\n",
        "        X = torch.Tensor(tokenized[:-1]).long().reshape(1, -1)\n",
        "        X = X.to('cuda')\n",
        "\n",
        "        y = torch.Tensor([tokenized[-1]]).long().to('cuda')\n",
        "\n",
        "        output, _ = model(X)\n",
        "        output = output[:, -1, :]\n",
        "\n",
        "        loss = criterion(output, y)\n",
        "        loss.backward()\n",
        "\n",
        "        running_loss += loss\n",
        "\n",
        "        if i % 2 == 0:\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            print('EPOCH: {}, BATCH: {}, LOSS: {}'.format(epoch, i, running_loss / 2))\n",
        "            running_loss = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now let's generate text based upon starting prompt"
      ],
      "metadata": {
        "id": "9NgV8q7Ufec-"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ze_t1bM2TZP"
      },
      "source": [
        "start_of_paper = 'Ivan thinks a lot about '"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrGiZ5xe2sOn"
      },
      "source": [
        "encoded_sequence = tokenizer.encode(start_of_paper)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "neAddEVy261Q",
        "outputId": "e9958bf7-f417-4dc5-94ab-0be9d657ce8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        }
      },
      "source": [
        "from pprint import pprint\n",
        "\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    while len(encoded_sequence) < 450:\n",
        "        predictions, _ = model(torch.Tensor([encoded_sequence]).long().to('cuda'))\n",
        "        prediction = predictions[:, -1, :]\n",
        "\n",
        "        # Ooooo, magic! (not really, but we won't have time to explain)\n",
        "        topk = torch.topk(prediction[0], 10)\n",
        "        values = F.softmax(topk.values, dim=0)\n",
        "        indices = topk.indices\n",
        "        indices_index = torch.multinomial(values, 1).item()\n",
        "        next_word_index = indices[indices_index].item()\n",
        "        \n",
        "        encoded_sequence.append(next_word_index)\n",
        "    \n",
        "pprint(tokenizer.decode(encoded_sequence))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('Ivan thinks a lot about that in a day. He has seen it and he had thought he '\n",
            " 'had lost something, he has lost the axe that is the axe that is in it and he '\n",
            " 'was not looking at it, he was looking at it, looking for the axe to have '\n",
            " 'returned from the bottom of a broken horn to one of his friends from out of '\n",
            " 'the same ring, but the axe came back and his life over again was in order '\n",
            " 'from the axe, he was alive and he was looking at the axe and the axe, it '\n",
            " 'seemed to himself to have come to a point where he could not take his axe to '\n",
            " 'do, it just seemed to himself to take a knife or to make one from one of his '\n",
            " 'own in a knife-edge axe, but he knew that he could not have been thinking of '\n",
            " 'that, he did not have a knife, he had not even a hammer, it was a straight '\n",
            " 'edge, he could not have a straight edge of axe, he could not even make a '\n",
            " 'straight edge with it in that, he could not make it by the knife and there '\n",
            " 'was nothing to make a straight edge in that with the axe from the axe that '\n",
            " 'was in his hand to do with, it seemed to himself to be straight and to be as '\n",
            " 'though he did from the axe that he was holding in his arms, he could not '\n",
            " 'make a straight edge from a blade of axe with it and he could not take one '\n",
            " 'with one hand, it had not been a knife, he did not know, he could not make a '\n",
            " 'knife with one hand with the axe that was in his arms with that, he did not '\n",
            " 'have any axe with one hand with the axe, he did not know that the edge was '\n",
            " 'bent at all one direction and to the top of the axe was bent, that it fell '\n",
            " 'straight, and that he could not bend the axe, he bent over, and the axe fell '\n",
            " 'from him again, he had been thinking that he could have broken the axe in '\n",
            " 'one hand, he did not know that, he bent over the axe, he was looking for his '\n",
            " 'axe in one hand and the axe was broken, he was still looking for something, '\n",
            " 'he did not have any axe in his arms, he did not know that')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fm1Xt_ptH9A5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "6cef6672-aeda-4c3a-88b3-cacaf3b3eddf"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tIuJsHR5nTq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "outputId": "a39be91b-d192-4a7d-a278-da8327df556c"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Mar  1 02:30:02 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.48.02    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P0    31W / 250W |   3997MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWnn0oH_7b1U"
      },
      "source": [
        "torch.save(model, 'drive/My Drive/test.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUAFjO4a7jaO"
      },
      "source": [
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EplNefds8O25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8ae61ab3-518e-43cf-d82f-6a2c5e66116e"
      },
      "source": [
        "os.listdir('drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.shortcut-targets-by-id', 'Shared drives', 'My Drive', '.Trash']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUmx7a9Z8QOu"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}